ğŸ¤– Human-Follow Ackermann Robot

A vision-driven crawler that follows humans using AI, built on Jetson Orin Nano + Teensy 4.1.


![WhatsApp Image 2025-08-18 at 19 15 42](https://github.com/user-attachments/assets/02f7cc42-53d5-4985-a9dd-1405bce4d2da)


âœ¨ Features

ğŸ” Vision AI â€“ DepthAI/PiNSIGHT camera running MobileNet-SSD

ğŸ§  Layered architecture â€“ Jetson for decision-making, Teensy for safety & actuation

ğŸ›¡ Built-in safety â€“ stall detection, impulse PWM, LiPo protection

ğŸ”Œ Simple protocol â€“ STOP / DRIVE / ARC / CENTER / PARAM with OK/DONE/TELEM

ğŸ›  Fully documented â€“ wiring table, BOM, CAD, and report included

ğŸ“‚ Repository Layout
firmware/     â†’ Teensy firmware + libraries (C++)
jetson/       â†’ Python code (vision, interface, decision layers)
cad/          â†’ STEP files (Fusion 360 exports)
docs/         â†’ Report, BOM, wiring, technical drawings
media/        â†’ Images, renders, GIFs (for README showcase)

ğŸ§© System Architecture


(Add a block diagram showing Vision â†’ Layer-5 â†’ Layer-4 â†’ Teensy â†’ Motors)

ğŸš€ Quickstart
Hardware

Teensy 4.1

Jetson Orin Nano 8GB

BTS7960 (drive), BTS7980 (steering)

DepthAI/PiNSIGHT camera

Dual 3S LiPos

ğŸ“‘ See BOM and Wiring Table.

Firmware (Teensy)
  Open firmware/RC_firmware_simple.ino
  Select board = Teensy 4.1
  Upload

Jetson / Python

cd jetson
python -m venv .venv && source .venv/bin/activate
pip install -r ../requirements.txt

Run

Safe STUB mode (no motors):

python Main.py


LIVE mode (real robot, wheels off ground first):
Edit Main.py â†’ MODE="LIVE"

python Main.py

ğŸ“¸ Media Showcase
Stage	Image
CAD Base	

Wiring	

Assembly	

Detection Preview	
ğŸ“– Documentation

ğŸ“‘ Project Report

ğŸ“¦ BOM

ğŸ”Œ Wiring Table

[ğŸ“ CAD + Drawings](cad/, docs/Final_Drawing.pdf)

ğŸ“œ License

MIT License â€“ free to use, modify, and build upon.
