ğŸ¤– Human-Follow Ackermann Robot

A vision-driven crawler that follows humans using AI, built on Jetson Orin Nano + Teensy 4.1.



https://github.com/user-attachments/assets/898d83e0-15fd-48f5-8a15-579c162862d4



âœ¨ Features

ğŸ” Vision AI â€“ DepthAI/PiNSIGHT camera running MobileNet-SSD

ğŸ§  Layered architecture â€“ Jetson for decision-making, Teensy for safety & actuation

ğŸ›¡ Built-in safety â€“ stall detection, impulse PWM, LiPo protection

ğŸ”Œ Simple protocol â€“ STOP / DRIVE / ARC / CENTER / PARAM with OK/DONE/TELEM

ğŸ›  Fully documented â€“ wiring table, BOM, CAD, and report included

ğŸ“‚ Repository Layout
firmware/     â†’ Teensy firmware + libraries (C++)
jetson/       â†’ Python code (vision, interface, decision layers)
cad/          â†’ STEP files (Fusion 360 exports)
docs/         â†’ Report, BOM, wiring, technical drawings
media/        â†’ Images, renders, GIFs (for README showcase)

ğŸ§© System Architecture

<img width="985" height="955" alt="human_follow_robot_architecture" src="https://github.com/user-attachments/assets/062f0b5e-b744-4acf-b0fd-9e938d87b004" />




ğŸš€ Quickstart
Hardware

Teensy 4.1

Jetson Orin Nano 8GB

BTS7960 (drive), BTS7980 (steering)

DepthAI/PiNSIGHT camera

Dual 3S LiPos

ğŸ“‘ See BOM and Wiring Table.

Firmware (Teensy)
  Open firmware/RC_firmware_simple.ino
  Select board = Teensy 4.1
  Upload

Jetson / Python

cd jetson
  python -m venv .venv && source .venv/bin/activate
  pip install -r ../requirements.txt

Run
  Safe STUB mode (no motors):
  python Main.py


LIVE mode (real robot, wheels off ground first):
Edit Main.py â†’ MODE="LIVE"

python Main.py

ğŸ“¸ Media Showcase
Stage	Image
CAD Base	

Wiring	

Assembly	

Detection Preview	
ğŸ“– Documentation

ğŸ“‘ Project Report

ğŸ“¦ BOM

ğŸ”Œ Wiring Table

[ğŸ“ CAD + Drawings](cad/, docs/Final_Drawing.pdf)

ğŸ“œ License

MIT License â€“ free to use, modify, and build upon.
